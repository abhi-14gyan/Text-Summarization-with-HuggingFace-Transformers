# Text-Summarization-with-HuggingFace-Transformers
Text Summarization with HuggingFace Transformers: Fine-Tuning

Implemented a complete NLP pipeline using T5ForConditionalGeneration from Hugging Face Transformers for text summarization, both with pre-trained inference and custom fine-tuning.

Fine-tuned the T5 model on domain-specific data using Google Colab, evaluating performance over 2 epochs with an evaluation loss of 1.66 and a runtime of 53.2 seconds.

Achieved evaluation throughput of 28.17 samples/sec and 3.53 steps/sec, demonstrating optimized resource utilization on limited hardware.

Built and compared summarization performance of zero-shot vs. fine-tuned models, analyzing gains in coherence and relevance on custom input data.

Developed deep practical knowledge of tokenization, attention-based models, training loops, and evaluation metrics for transformer-based NLP models.

Utilized Hugging Faceâ€™s Trainer API to streamline training, logging, and performance tracking for experimentation.
